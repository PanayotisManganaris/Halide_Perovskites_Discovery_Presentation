% Created 2022-06-21 Tue 10:32
% Intended LaTeX compiler: pdflatex
\documentclass[10pt, compress]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\institute[Mannodi Group]{\inst{1} Purdue Materials Science and Engineering Mannodi Group}
\mode<beamer>{\usetheme{Warsaw}}
\useoutertheme{miniframes}
\setbeamertemplate{caption}[numbered]
%
\usepackage[%
%citestyle=authoryear-icomp,
%bibstyle=authoryear,
style=verbose,
hyperref=true,
backref=true,
maxcitenames=3,
url=true,
backend=biber,
natbib=true,
]{biblatex}
\addbibresource{~/org/bibliotex/bibliotex.bib}
\usetheme{default}
\author{Panayotis Manganaris\inst{1}}
\date{\today{}}
\title{Statistical Learning for Halide Perovskite Discovery}
\hypersetup{
 pdfauthor={Panayotis Manganaris\inst{1}},
 pdftitle={Statistical Learning for Halide Perovskite Discovery},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 29.0.50 (Org mode 9.5.3)}, 
 pdflang={English}}
\begin{document}

\maketitle
\begin{frame}{Outline}
\tableofcontents
\end{frame}

\expandafter\def\expandafter\insertshorttitle\expandafter{%
  \insertshorttitle\hfill
  \insertframenumber\,/\,\inserttotalframenumber}
\section{AI Background}
\label{sec:org0f7a1a6}
\begin{frame}[label={sec:org6b8314f}]{Artificial Intelligence}
\begin{block}{The Four Approached to AI}
\begin{center}
\begin{tabular}{ll}
Thinking Humanly & Thinking Rationally\\
- Turing test approach & - Laws of Thought\\
(The Six Fields of AI)\footnotemark & -- logical positing\\
-- NLP & -- proven algorithms\\
-- Knowledge Representation & -- correct inference\\
-- automated reasoning & -- syllogistic reason\\
-- Machine Learning & \\
-- computer vision & \\
-- robotics & \\
\hline
\hline
Acting Humanly & Acting Rationally\\
- cognitive modeling approach & - The rational agent\\
-- neuromorphic algorithms & -- inference + reflex\\
 & -- inference vs deduction\\
\end{tabular}
\end{center}\footnotetext[1]{\label{org884469e}\cite{russell-2010-artif}}
\end{block}
\end{frame}
\begin{frame}[allowframebreaks]{Machine Learning}
\begin{block}{ML Contributes to AI}
\begin{itemize}
\item Adaptable \alert{agent}
\begin{itemize}
\item Contextual judgment of \alert{percept} relevance
\item Autonomous utilization of \alert{percept sequence}
\end{itemize}
\item Learning
\begin{itemize}
\item \alert{function} performance improves with exposure to more percepts
\end{itemize}
\end{itemize}
\end{block}
\begin{definition}[Artifical Agency]
\begin{description}
\item[{agent}] self-contained sensor->function->action pipeline
\item[{function}] Set of all possible responses for all possible percepts
\item[{percept}] sensory input
\item[{percept sequence}] history of sensory input
\end{description}
\end{definition}
\begin{block}{Supervised Training}
Encourage the agent to behave "correctly"
\begin{enumerate}
\item Minimize Loss
\item Maximize Score
\end{enumerate}
\end{block}
\begin{block}{Unsupervised Training}
The agent determines something principally true about its environment
using mathematical/logical characterization methods.
\begin{itemize}
\item find eigenvectors and eigenvalues
\item differentially calculate optima
\end{itemize}
\end{block}
\end{frame}
\begin{frame}[label={sec:orgce37025}]{Inverse Design}
\begin{block}{A Type of AI Implementation}
\begin{description}
\item[{senses}] maps points in many dimensions
\item[{function}] reliably navigates it's environment searching for optima
\item[{action}] returns its findings to human interpreters
\end{description}
\end{block}
\end{frame}
\section{Chemistry Background}
\label{sec:org61d5189}
\begin{frame}[label={sec:org1ed4f13}]{Perovskite Structure and Chemistry}
Example\footnote{\cite{mannodi-kanakkithodi-2022-data-driven}} of hybrid organic-inorganic MAPbI\textsubscript{3}
\begin{center}
\includegraphics[width=190]{cubic_perovskite.png}
\end{center}
\end{frame}
\begin{frame}[label={sec:org887e397}]{Our Dataset}
\begin{columns}
\begin{column}{0.35\columnwidth}
\begin{block}{DFT Simulations}
\begin{enumerate}
\item geometry optimization
\item Static band structure and optical absorption
\end{enumerate}
\end{block}
\begin{block}{Levels of Theory}
\begin{itemize}
\item PBE
\item HSE06
\item PBE+HSE06(SOC)
\item Experimental
\end{itemize}
\end{block}
\end{column}
\begin{column}{0.65\columnwidth}
\begin{center}
\begin{tabular}{lrrl}
Formula & bg\textsubscript{eV} & \(\eta\) & LoT\\
\hline
MAPbCl3 & 3.03 & 0.002 & EXP\\
CsPbI0.375Br2.625 & 1.68 & 0.153 & PBE\\
RbSnBr2.625Cl0.375 & 1.44 & NaN & HSE\\
CsGeCl3 & 1.05 & 0.176 & PBE\\
MASr0.5Pb0.5Cl3 & 5.31 & NaN & HSE\\
MABa0.25Pb0.75I3 & 1.99 & 0.015 & PBE\\
MASnI3 & 2.57 & NaN & HSE\\
MACa0.5Pb0.5Cl3 & 5.32 & NaN & HSE\\
\ldots{} & \ldots{} & \ldots{} & \ldots{}\\
\end{tabular}
\end{center}
\end{column}
\end{columns}
\end{frame}
\begin{frame}[allowframebreaks]{Band Gap Fidelity}
\begin{figure}[htbp]
\centering
\includegraphics[width=250]{pbe_v_hse_bg.png}
\caption{PBE vs HSE Band Gaps}
\end{figure}
Comparing computational with experimental\footnote{\cite{almora-2020-devic-perfor}} band gaps
\begin{columns}
\begin{column}{0.4\columnwidth}
\begin{figure}[htbp]
\centering
\includegraphics[height=110]{pbe_v_almora_bg.png}
\caption{PBE vs Almora BG}
\end{figure}
\end{column}
\begin{column}{0.6\columnwidth}
\begin{figure}[htbp]
\centering
\includegraphics[height=110]{hse_v_almora_bg.png}
\caption{HSE vs Almora BG}
\end{figure}
\end{column}
\end{columns}
\end{frame}
\section{Pipeline}
\label{sec:orgc1d00d5}
\begin{frame}[label={sec:org5af1f67}]{Data Pre-Processing}
\begin{figure}[htbp]
\centering
\includegraphics[width=300]{./data_proc.png}
\caption{Data Preprocessing Workflow to Implement with Python Pandas}
\end{figure}
\end{frame}

\begin{frame}[label={sec:org11570a5}]{Machine Learning Pipeline}
\begin{figure}[htbp]
\centering
\includegraphics[width=300]{./ML_pipe.png}
\caption{Machine Learning Pipeline to Implement with Python SciKit-Learn}
\end{figure}
\end{frame}

\begin{frame}[fragile,allowframebreaks]{Implementation in Jupyter Python}
 \lstset{language=Python,label= ,caption={Scikit-Learn mock-setup and pandas data loading + grouping},captionpos=b,numbers=none}
\begin{lstlisting}
import sys, os
sys.path.append(os.path.expanduser("~/src/cmcl"))
sys.path.append(os.path.expanduser("~/src/spyglass"))
import pandas as pd
import numpy as np
import cmcl
from spyglass.model_imaging import parityplot
from sklearn.pipeline import make_pipeline
from sklearn.compose import ColumnTransformer
from sklearn.<module> import NumPreProcessor1
from sklearn.<module> import CatPreProcessor1
from sklearn.<module> import NumPreProcessor2
from sklearn.<module> import CatPreProcessor2
from sklearn.<module> import Estimator

df = pd.read_<data>('./file.<data>')
df = df.groupby('Formula', as_index=False).agg(
    {'bg_eV':'median', 'efficiency':'median'})
\end{lstlisting}

\lstset{language=Python,label= ,caption={Computing Features with cmcl and Feature Engineering},captionpos=b,numbers=none}
\begin{lstlisting}
dc = df.ft.comp()
dc = dc.assign(label='label')

numeric_features = dc
.select_dtypes(np.number)
.columns
.to_list()
numeric_pipeline = make_pipeline(NumPreProcessor1(),
				 NumPreProcessor2())
categorical_features = mc
.select_dtypes('object')
.columns
.to_list()
catagorical_pipeline = make_pipeline(CatPreProcessor1(),
				     CatPreProcessor2())
\end{lstlisting}

\lstset{language=Python,label= ,caption={make Test/Train Split, Assemble, and Fit pipeline},captionpos=b,numbers=none}
\begin{lstlisting}
ss = ShuffleSplit(n_splits=1, train_size=0.8,
		  random_state=None)
train_idx, test_idx = next(ss.split(dc))
dc_tr, dc_ts = dc.iloc[train_idx], dc.iloc[test_idx]
df_tr, df_ts = df.iloc[train_idx], df.iloc[test_idx]

preprocessor = ColumnTransformer(
    transformers=[
	("num", numeric_pipeline, numeric_features),
	("cat", categorical_pipline, categorical_features),
    ]
)

pipe = make_pipeline(preprocessor, Estimator())

pipe.fit(dc_tr, df_tr.<target>)
\end{lstlisting}

\lstset{language=Python,label= ,caption={Evaluate Pipeline using Spyglass},captionpos=b,numbers=none}
\begin{lstlisting}
p, data = parityplot(pipe,
		     dc_ts, df_ts.<target>.to_frame(),
		     aspect=1.0)
p.figure.show()
\end{lstlisting}
\end{frame}

\section{Feature Engineering}
\label{sec:orgd2d569e}
\begin{frame}[label={sec:org71bf433}]{PCA}
\begin{columns}
\begin{column}{0.75\columnwidth}
\begin{figure}[htbp]
\centering
\includegraphics[width=200]{comp_ratio_projection.png}
\caption{Learn transformation matrix \(U\) to diagonalizes the matrix \(A\). The Principal Components in \(Q\) corresponding to the largest two Singular Values in \(S\) contain the majority of the variance in the data.}
\end{figure}
\end{column}
\begin{column}{0.25\columnwidth}
\[
UAU^\dag = Q^{-1}SQ
\]
\end{column}
\end{columns}
\end{frame}
\begin{frame}[label={sec:org300ba6c}]{tSNE}
\begin{figure}[htbp]
\centering
\includegraphics[width=200]{tsne_comp_DecoE_clusters.png}
\caption{Learn a low-dimensional (2 or 3D) embedding space in which statistical similarity governs the proximity of high-dimensional data points}
\end{figure}
\end{frame}
\begin{frame}[label={sec:org4d3186f}]{UMAP}
\begin{figure}[htbp]
\centering
\includegraphics[width=200]{umap_projection.png}
\caption{Learn a manifold embedding space in which nearest neighbors form clusters}
\end{figure}
\end{frame}
\section{Supervised Architectures}
\label{sec:org6c21e05}
\begin{frame}[allowframebreaks]{Linear regressions on BG}
\begin{figure}[htbp]
\centering
\includegraphics[width=185]{linear_bg_c.png}
\caption{OLS determines \(\vec{w}\) so that \(f(x) = \vec{x}^T\vec{w}\), \(y_i = f(x_i) + \epsilon_i\) and all \(\epsilon\)\textsubscript{i} are as small as possible}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=185]{elastic_bg_c.png}
\caption{elasticnet determines \(\vec{w}\) as before, but also works to sparsify the model}
\end{figure}
\end{frame}
\begin{frame}[label={sec:org6d7f893}]{OLS weights}
\begin{columns}
\begin{column}{0.7\columnwidth}
\begin{center}
\begin{tabular}{llr}
site & element & \\
\hline
A & Cs & 23.771206\\
A & FA & 25.794831\\
A & K & 22.774475\\
A & MA & 25.452629\\
A & Rb & 23.282988\\
B & Ba & -32.603053\\
B & Ca & -31.378385\\
B & Ge & -45.001044\\
B & Pb & -42.526511\\
B & Sn & -46.868114\\
B & Sr & -32.068490\\
X & Br & 0.939374\\
X & Cl & 1.769032\\
X & I & 0.140658\\
\end{tabular}
\end{center}
.
\end{column}
\begin{column}{0.3\columnwidth}
\begin{center}
\begin{tabular}{lr}
 & RSS\\
\hline
A & 54.213044\\
B & 95.426246\\
X & 2.007905\\
\end{tabular}
\end{center}
\end{column}
\end{columns}
\end{frame}

\begin{frame}[label={sec:org6452e0d}]{elasticnet weights}
\begin{columns}
\begin{column}{0.7\columnwidth}
\begin{center}
\begin{tabular}{llr}
site & element & \\
\hline
A & Cs & -0.191057\\
A & FA & 1.589015\\
A & K & -1.081903\\
A & MA & 1.214167\\
A & Rb & -0.530437\\
B & Ba & 5.139688\\
B & Ca & 6.424156\\
B & Ge & -5.879154\\
B & Pb & -3.673012\\
B & Sn & -7.689152\\
B & Sr & 5.678253\\
X & Br & 0.000000\\
X & Cl & 0.819669\\
X & I & -0.786942\\
\end{tabular}
\end{center}
\end{column}
\begin{column}{0.3\columnwidth}
\begin{center}
\begin{tabular}{lr}
 & RSS\\
\hline
A & 2.342552\\
B & 14.391222\\
X & 1.136281\\
\end{tabular}
\end{center}
\end{column}
\end{columns}
\end{frame}

\begin{frame}[allowframebreaks]{Random Forest Regression on BG}
\begin{figure}[htbp]
\centering
\includegraphics[width=170]{rfr_c_bg.png}
\caption{RFR initializes an ensemble of Decision Trees and averages their results to return its prediction. This leverages the DT's ability to strongly bias itself to the data and relies on randomness to explain variance in the underlying process}
\end{figure}
\end{frame}
\subsection{RFR Feature Importance}
\label{sec:org6403c1f}
\subsection{}
\label{sec:org859dc4a}
\begin{frame}[allowframebreaks]{Gaussian Process or BG}
\begin{figure}[htbp]
\centering
\includegraphics[width=185]{gpr_bg_c.png}
\caption{GPR picks functions from a distribution derived from the data covariance. The functions that satisfy the data form the fit.}
\end{figure}
\begin{block}{Regularization with Priors}
\begin{description}
\item[{Conditional Probablity}] \(P(x|y) = \frac{P(x)P(y|x)}{P(x)}\)
\item[{Conditional Odds}] \(O(x|y) = O(x)\frac{P(x|y)}{P(x|\neg{}y)}\)
\item[{Isolated Bayesian Prior}] \(B = \frac{P(x|y)}{P(x|\neg{}y)}\)
\end{description}
\end{block}
\end{frame}

\section{}
\label{sec:org2564950}
\printbibliography
\end{document}